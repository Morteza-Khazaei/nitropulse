"""Plotting utilities for validating soil moisture estimates.

These helpers are designed to be imported inside notebooks. They accept
dataframes generated by the nitropulse pipeline (RISMA observations and
model holdout predictions) and emit Plotly figures mirroring the
validation visuals used during development.
"""

from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime
from typing import Iterable, Optional, Tuple, Union
import warnings

import numpy as np
import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots

from pathlib import Path

DATE_TICKFORMAT = "%b %Y"
FIGURE_SUBDIR = "figures"

FONT_FAMILY_BASE = "DejaVu Sans, Helvetica, Arial, sans-serif"
FONT_FAMILY_BOLD = "DejaVu Sans Bold, DejaVu Sans, Helvetica, Arial, sans-serif"
LABEL_FONT_SIZE = 14
TICK_FONT_SIZE = 12
TITLE_FONT_SIZE = 18

LEGEND_FONT = dict(size=LABEL_FONT_SIZE, family=FONT_FAMILY_BOLD)
AXIS_TITLE_FONT = dict(size=LABEL_FONT_SIZE, family=FONT_FAMILY_BOLD)
TITLE_FONT = dict(size=TITLE_FONT_SIZE, family=FONT_FAMILY_BOLD)
TICK_FONT = dict(size=TICK_FONT_SIZE, family=FONT_FAMILY_BASE)


def _as_list(value: Optional[Union[Iterable, str, int]]) -> Optional[list]:
    if value is None:
        return None
    if isinstance(value, (list, tuple, set, pd.Series, np.ndarray)):
        return list(value)
    return [value]


def _apply_filter(frame: pd.DataFrame, col: str, value) -> pd.DataFrame:
    values = _as_list(value)
    if values is None or col not in frame:
        return frame
    return frame[frame[col].isin(values)]


@dataclass
class Metrics:
    n: int
    r: float
    rmse: float
    ubrmse: float
    bias: float
    slope: float
    intercept: float


def build_holdout_subset(
    test_df: pd.DataFrame,
    station: Optional[Iterable[str] | str] = None,
    year: Optional[Iterable[int] | int] = None,
    *,
    crop: Optional[Iterable[str] | str] = None,
    target: str = "ssm",
    station_col: str = "station",
    year_col: str = "year",
    crop_col: str = "lc",
) -> pd.DataFrame:
    """Return a filtered view of ``test_df`` for the requested criteria.

    Parameters
    ----------
    test_df : pandas.DataFrame
        Holdout records produced by :class:`RegressionRF`. Must contain the
        columns specified below.
    station, year, crop : str or iterable, optional
        Values to filter by. ``None`` keeps all values for that dimension.
    target : str, default ``"ssm"``
        Name of the reference target column. A ``"<target>_pred"`` column is
        expected for the predictions.
    station_col, year_col, crop_col : str
        Column names that store the station and year information.
    """

    pred_col = f"{target}_pred"
    if target not in test_df or pred_col not in test_df:
        missing = {target, pred_col} - set(test_df.columns)
        raise KeyError(
            f"Holdout dataframe must contain {target!r} and {pred_col!r}; missing {missing}."
        )

    subset = test_df.copy()

    subset = _apply_filter(subset, station_col, station)
    subset = _apply_filter(subset, year_col, year)
    subset = _apply_filter(subset, crop_col, crop)

    if subset.empty:
        raise ValueError(
            "No records found for the specified station/year/crop filters."
        )
    sort_key = "doy" if "doy" in subset.columns else "date" if "date" in subset.columns else None
    if sort_key is not None:
        subset = subset.sort_values(sort_key)
    return subset


def _compute_metrics(obs: Iterable[float], pred: Iterable[float]) -> Metrics:
    obs_arr = np.asarray(list(obs), dtype=float)
    pred_arr = np.asarray(list(pred), dtype=float)
    mask = ~np.isnan(obs_arr) & ~np.isnan(pred_arr)
    obs_arr = obs_arr[mask]
    pred_arr = pred_arr[mask]
    if obs_arr.size == 0:
        raise ValueError("Cannot compute metrics with no valid pairs.")

    diff = obs_arr - pred_arr
    rmse = np.sqrt(np.mean(diff**2))
    bias = np.mean(pred_arr - obs_arr)
    ubrmse = np.sqrt(max(rmse**2 - bias**2, 0))

    if obs_arr.size < 2:
        r = np.nan
        slope = np.nan
        intercept = np.nan
    else:
        r = np.corrcoef(obs_arr, pred_arr)[0, 1]
        slope, intercept = np.polyfit(obs_arr, pred_arr, 1)

    return Metrics(
        n=obs_arr.size,
        r=r,
        rmse=rmse,
        ubrmse=ubrmse,
        bias=bias,
        slope=slope,
        intercept=intercept,
    )


def _format_metrics(metrics: Metrics) -> list[str]:
    return [
        f"n = {metrics.n}",
        f"r = {metrics.r:.3f}" if not np.isnan(metrics.r) else "r = n/a",
        f"RMSE = {metrics.rmse:.4f}",
        f"ubRMSE = {metrics.ubrmse:.4f}",
        f"Bias = {metrics.bias:+.4f}",
    ]


def _get_palette(n: int) -> list[str]:
    base = [
        "#1f77b4",
        "#ff7f0e",
        "#2ca02c",
        "#d62728",
        "#9467bd",
        "#8c564b",
        "#e377c2",
        "#7f7f7f",
        "#bcbd22",
        "#17becf",
    ]
    if n <= len(base):
        return base[:n]
    repeats = int(np.ceil(n / len(base)))
    return (base * repeats)[:n]


def _legend_entry_count(fig: go.Figure) -> int:
    """Count traces that will display in the legend."""

    count = 0
    for trace in fig.data:
        showlegend = getattr(trace, "showlegend", None)
        if showlegend is None:
            showlegend = True
        if showlegend:
            count += 1
    return count


def _ensure_vertical_legend_fit(
    fig: go.Figure,
    *,
    font_size: Optional[int] = None,
    width_estimate: int = 220,
    height_padding: int = 150,
    min_right_margin: int = 260,
    right_padding: int = 100,
) -> None:
    """Ensure vertical legends have enough canvas space, especially for export."""

    legend = getattr(fig.layout, "legend", None)
    if legend is None:
        return
    orientation = getattr(legend, "orientation", "v")
    if orientation != "v":
        return

    legend_font = getattr(legend, "font", None)
    font_size = font_size or (getattr(legend_font, "size", None) or 12)

    entry_count = max(1, _legend_entry_count(fig))
    approx_line_height = font_size + 6
    legend_height = 30 + approx_line_height * entry_count

    current_height = getattr(fig.layout, "height", None)
    margin = getattr(fig.layout, "margin", None)
    margin_l = getattr(margin, "l", None)
    margin_r = getattr(margin, "r", None)
    margin_t = getattr(margin, "t", None)
    margin_b = getattr(margin, "b", None)

    if margin_l is None:
        margin_l = 60
    if margin_r is None:
        margin_r = 60
    if margin_t is None:
        margin_t = 60
    if margin_b is None:
        margin_b = 60

    required_height = legend_height + height_padding + margin_t + margin_b
    if current_height is None or current_height < required_height:
        fig.update_layout(height=required_height)

    required_r = max(margin_r, min_right_margin, width_estimate + right_padding)

    fig.update_layout(
        margin=dict(l=margin_l, r=required_r, t=margin_t, b=margin_b)
    )


def _figure_root(workspace_dir: Optional[Union[str, Path]]) -> Path:
    root = Path(workspace_dir).expanduser() if workspace_dir else (Path.home() / ".nitropulse")
    base_dir = root / FIGURE_SUBDIR
    base_dir.mkdir(parents=True, exist_ok=True)
    return base_dir


def _resolve_figure_path(
    filename: Optional[Union[str, Path]],
    prefix: str,
    *,
    workspace_dir: Optional[Union[str, Path]] = None,
    extension: str,
) -> Path:
    base_dir = _figure_root(workspace_dir)

    if filename:
        path = Path(filename)
        if not path.is_absolute():
            path = base_dir / path
    else:
        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        path = base_dir / f"{prefix}_{timestamp}.{extension}"

    suffix = f".{extension.lstrip('.')}"
    return path.with_suffix(suffix)


def _bold_text(text: Optional[str]) -> Optional[str]:
    if not text:
        return text
    stripped = str(text)
    if stripped.startswith("<b>") and stripped.endswith("</b>"):
        return stripped
    return f"<b>{stripped}</b>"


def _save_figure(
    fig: go.Figure,
    filename: Optional[str],
    prefix: str,
    *,
    workspace_dir: Optional[Union[str, Path]] = None,
    width: Optional[int] = None,
    height: Optional[int] = None,
) -> Optional[Path]:
    """Persist ``fig`` as a PNG within ``<workspace>/figures`` and return the path."""

    if fig is None:
        return None

    path = _resolve_figure_path(filename, prefix, workspace_dir=workspace_dir, extension="png")

    write_kwargs = {}
    if width is not None:
        write_kwargs["width"] = width
    if height is not None:
        write_kwargs["height"] = height

    fig_to_save = go.Figure(fig)

    try:
        _ensure_date_tickformat(fig_to_save)
        _normalize_datetime_traces(fig_to_save)
        _ensure_vertical_legend_fit(fig_to_save)
        fig_to_save.write_image(str(path), **write_kwargs)
    except (ValueError, RuntimeError) as exc:  # pragma: no cover - runtime dependency
        message = str(exc)
        if "Chrome" in message or "kaleido" in message.lower() or "Browser" in message:
            warnings.warn(
                "Figure export skipped: Plotly's Kaleido engine (with Chrome dependencies) "
                "is not available. Install `kaleido` and run `plotly_get_chrome` to enable PNG saving.",
                RuntimeWarning,
            )
            return None
        warnings.warn(f"Figure export skipped due to unexpected error: {message}", RuntimeWarning)
        return None

    return path


def _save_figure_html(
    fig: go.Figure,
    filename: Optional[str],
    prefix: str,
    *,
    workspace_dir: Optional[Union[str, Path]] = None,
    include_plotlyjs: Union[bool, str] = "cdn",
    full_html: bool = True,
    config: Optional[dict] = None,
    default_width: Optional[int] = None,
    default_height: Optional[int] = None,
) -> Optional[Path]:
    """Persist ``fig`` as an interactive HTML file and return the path."""

    if fig is None:
        return None

    path = _resolve_figure_path(filename, prefix, workspace_dir=workspace_dir, extension="html")

    fig_to_save = go.Figure(fig)
    try:
        fig_to_save.write_html(
            str(path),
            include_plotlyjs=include_plotlyjs,
            full_html=full_html,
            config=config,
            default_width=default_width,
            default_height=default_height,
        )
    except Exception as exc:  # pragma: no cover - runtime dependent
        warnings.warn(
            f"Figure HTML export skipped due to unexpected error: {exc}",
            RuntimeWarning,
        )
        return None

    return path


def _ensure_date_tickformat(fig: go.Figure) -> None:
    axis_updates: dict[str, dict[str, str]] = {}
    for key in dir(fig.layout):
        if not key.startswith("xaxis"):
            continue
        axis = getattr(fig.layout, key, None)
        if axis is None:
            continue
        axis_type = getattr(axis, "type", None)
        tickformat = getattr(axis, "tickformat", None)
        if axis_type == "date":
            if not tickformat:
                axis_updates.setdefault(key, {})["tickformat"] = DATE_TICKFORMAT
        elif axis_type in (None, "-"):
            # Assume datetime-friendly axis if traces supplied timestamps.
            if _axis_has_datetime_data(fig, key):
                updates = axis_updates.setdefault(key, {})
                updates["type"] = "date"
                if not tickformat:
                    updates["tickformat"] = DATE_TICKFORMAT
    if axis_updates:
        fig.update_layout(**axis_updates)


def _normalize_datetime_traces(fig: go.Figure) -> None:
    for trace in fig.data:
        values = getattr(trace, "x", None)
        if not _sequence_is_datetime(values):
            continue
        normalized = _to_datetime_list(values)
        setattr(trace, "x", normalized)


def _axis_has_datetime_data(fig: go.Figure, axis_key: str) -> bool:
    axis_ref = "x" if axis_key == "xaxis" else axis_key.replace("xaxis", "x")
    for trace in fig.data:
        trace_axis = getattr(trace, "xaxis", None)
        if trace_axis and trace_axis != axis_ref:
            continue
        values = getattr(trace, "x", None)
        if _sequence_is_datetime(values):
            return True
    return False


def _sequence_is_datetime(values) -> bool:
    if values is None:
        return False
    if isinstance(values, (pd.Series, pd.Index)):
        return pd.api.types.is_datetime64_any_dtype(values)
    if isinstance(values, np.ndarray):
        if values.size == 0:
            return False
        if np.issubdtype(values.dtype, np.datetime64):
            return True
        values = values.tolist()
    if isinstance(values, (list, tuple)):
        for val in values:
            if val is None:
                continue
            if isinstance(val, (pd.Timestamp, np.datetime64, datetime)):
                return True
            if hasattr(val, "to_pydatetime"):
                try:
                    val.to_pydatetime()
                    return True
                except Exception:
                    continue
    return False


def _to_datetime_list(values) -> list:
    if isinstance(values, pd.Series):
        return [pd.to_datetime(v).to_pydatetime() if not pd.isna(v) else None for v in values]
    if isinstance(values, pd.Index):
        return [pd.to_datetime(v).to_pydatetime() if not pd.isna(v) else None for v in values]
    if isinstance(values, np.ndarray):
        values = values.tolist()
    normalized = []
    for val in values:
        if val is None:
            normalized.append(None)
            continue
        if isinstance(val, datetime):
            normalized.append(val)
            continue
        if isinstance(val, np.datetime64):
            normalized.append(pd.to_datetime(val).to_pydatetime())
            continue
        if isinstance(val, pd.Timestamp):
            normalized.append(val.to_pydatetime())
            continue
        if hasattr(val, "to_pydatetime"):
            try:
                normalized.append(val.to_pydatetime())
                continue
            except Exception:
                pass
        normalized.append(pd.to_datetime(val).to_pydatetime())
    return normalized


def figure_export_kwargs(
    name: Optional[Union[str, Path]] = None,
    *,
    fmt: Optional[str] = "png",
    workspace_dir: Optional[Union[str, Path]] = None,
) -> dict:
    """Return keyword arguments for scatter/timeseries helpers to export figures.

    Parameters
    ----------
    name : str or path-like, optional
        Base filename (without extension) for the figure. ``None`` delegates to
        the plotting helper's default timestamped filename.
    fmt : {"png", "html"}, optional
        Output format. ``"png"`` triggers the static export, ``"html"`` produces
        an interactive Plotly HTML bundle.
    workspace_dir : path-like, optional
        Root directory that contains the ``figures`` subfolder. Defaults to the
        same location used by :func:`plot_scatter` and :func:`plot_timeseries`
        (``~/.nitropulse/figures``) when omitted.
    """

    fmt_normalized = (fmt or "png").lower()
    if fmt_normalized not in {"png", "html"}:
        raise ValueError("fmt must be 'png' or 'html'.")

    export_name: Optional[str]
    if name is None:
        export_name = None
    else:
        export_name = Path(name).stem if str(name).strip() else None

    kwargs: dict = {"workspace_dir": workspace_dir}
    if fmt_normalized == "html":
        html_payload: dict[str, Optional[Union[str, bool]]] = {"include_plotlyjs": "cdn"}
        if export_name:
            html_payload["name"] = export_name
        kwargs["save_html"] = html_payload
    else:
        kwargs["save"] = export_name if export_name else True
    return kwargs


def figure_output_path(
    name: Optional[Union[str, Path]] = None,
    *,
    fmt: Optional[str] = "png",
    workspace_dir: Optional[Union[str, Path]] = None,
    prefix: str = "figure",
) -> Path:
    """Compute the resolved filesystem path for a figure export."""

    fmt_normalized = (fmt or "png").lower()
    extension = "html" if fmt_normalized == "html" else "png"
    filename = Path(name).stem if name else None
    return _resolve_figure_path(filename, prefix, workspace_dir=workspace_dir, extension=extension)


def plot_scatter(
    obs_df: pd.DataFrame,
    holdout_df: pd.DataFrame,
    *,
    station: Optional[Iterable[str] | str] = None,
    year: Optional[Iterable[int] | int] = None,
    crop: Optional[Iterable[str] | str] = None,
    target: str = "ssm",
    station_col: str = "station",
    year_col: str = "year",
    crop_col: str = "lc",
    group_cols: Optional[Iterable[str]] = None,
    title_text: Optional[str] = None,
    save: Union[bool, str] = False,
    save_html: Union[bool, str, dict] = False,
    save_width: Optional[int] = None,
    save_height: Optional[int] = None,
    workspace_dir: Optional[Union[str, Path]] = None,
) -> go.Figure:
    """Create a scatter validation plot for any combination of filters.

    Automatically determines the grouping (colour/style) based on which of
    ``station``, ``year`` and ``crop`` still vary after filtering, unless
    ``group_cols`` is provided explicitly.
    """

    subset = build_holdout_subset(
        holdout_df,
        station=station,
        year=year,
        crop=crop,
        target=target,
        station_col=station_col,
        year_col=year_col,
        crop_col=crop_col,
    )

    if target not in subset.columns or subset[target].isna().all():
        if target not in obs_df.columns:
            raise KeyError(f"Target column '{target}' not found in observation dataframe.")
        obs_subset = build_holdout_subset(
            obs_df,
            station=station,
            year=year,
            crop=crop,
            target=target,
            station_col=station_col,
            year_col=year_col,
            crop_col=crop_col,
        )
        merge_keys = [col for col in [station_col, year_col, crop_col, 'doy', 'date'] if col in subset.columns and col in obs_subset.columns]
        if not merge_keys:
            raise ValueError("Unable to align observation and prediction dataframes; ensure shared keys such as station/year/doy or date are present.")
        obs_subset = obs_subset[merge_keys + [target]]
        subset = subset.merge(obs_subset, on=merge_keys, how='left', suffixes=('', '_obs'))
        obs_column = f"{target}_obs"
        if obs_column in subset.columns:
            subset[target] = subset[target].fillna(subset.pop(obs_column))

    pred_col = f"{target}_pred"

    def _single_value_from_filter(value):
        values = _as_list(value)
        if not values:
            return None
        cleaned = []
        for item in values:
            try:
                if pd.isna(item):
                    continue
            except Exception:
                pass
            cleaned.append(item)
        if not cleaned:
            return None
        deduped = list(dict.fromkeys(cleaned))
        return deduped[0] if len(deduped) == 1 else None

    def _normalize_label_value(value):
        if value is None:
            return None
        try:
            if pd.isna(value):
                return None
        except Exception:
            pass
        if isinstance(value, (np.integer, np.int64, np.int32)):
            return int(value)
        if isinstance(value, (np.floating, float)):
            if np.isnan(value):
                return None
            return int(value) if float(value).is_integer() else float(value)
        return value

    user_category_cols = group_cols
    if user_category_cols is not None:
        category_cols = [
            col for col in dict.fromkeys(_as_list(user_category_cols) or []) if col in subset.columns
        ]
    else:
        category_cols = []
        if year_col in subset.columns:
            category_cols.append(year_col)

    if not category_cols and station_col in subset.columns:
        category_cols.append(station_col)

    # Ensure uniqueness while preserving order
    category_cols = [c for c in dict.fromkeys(category_cols) if c in subset.columns]

    groupby_cols: list[str] = []
    for col in category_cols:
        if col in subset.columns and col not in groupby_cols:
            groupby_cols.append(col)
    for col in (station_col, year_col, crop_col):
        if col in subset.columns and col not in groupby_cols:
            groupby_cols.append(col)

    fig = go.Figure()
    seen_legend_groups: set[str] = set()

    # Determine plotting ranges across all points once
    xy_min = float(np.nanmin(subset[[target, pred_col]].values))
    xy_max = float(np.nanmax(subset[[target, pred_col]].values))
    if not np.isfinite(xy_min) or not np.isfinite(xy_max):
        raise ValueError("Cannot determine axis limits due to non-finite values.")
    if xy_min == xy_max:
        xy_min -= 0.01
        xy_max += 0.01
    else:
        pad = (xy_max - xy_min) * 0.05
        xy_min -= pad
        xy_max += pad

    if not groupby_cols:
        metrics = _compute_metrics(subset[target], subset[pred_col])
        trace_name = (
            f"station={station or 'all'} | lc={crop or 'all'} | year={year or 'all'} (n={metrics.n}, r={metrics.r:.3f}, ubRMSE={metrics.ubrmse:.4f}, bias={metrics.bias:+.4f})"
        )
        fig.add_trace(
            go.Scatter(
                x=subset[target],
                y=subset[pred_col],
                mode="markers",
                marker=dict(color="#1f1f20", size=7, opacity=0.9),
                name=trace_name,
            )
        )
    else:
        grouped = subset.groupby(groupby_cols, dropna=False)
        palette = _get_palette(len(grouped))
        for idx, (key, group) in enumerate(grouped):
            metrics = _compute_metrics(group[target], group[pred_col])
            if isinstance(key, tuple):
                key_map = {col: val for col, val in zip(groupby_cols, key)}
            else:
                key_map = {groupby_cols[0]: key}

            station_label = _normalize_label_value(key_map.get(station_col))
            if station_label is None:
                station_label = _normalize_label_value(_single_value_from_filter(station))

            crop_label = _normalize_label_value(key_map.get(crop_col))
            if crop_label is None:
                crop_label = _normalize_label_value(_single_value_from_filter(crop))

            year_label = _normalize_label_value(key_map.get(year_col))
            if year_label is None:
                year_label = _normalize_label_value(_single_value_from_filter(year))

            legend_group_parts: list[str] = []
            for col in category_cols:
                col_value = _normalize_label_value(key_map.get(col))
                if col == station_col and col_value is None:
                    col_value = station_label
                elif col == crop_col and col_value is None:
                    col_value = crop_label
                elif col == year_col and col_value is None:
                    col_value = year_label
                if col_value is None:
                    continue
                legend_group_parts.append(str(col_value))
            legend_group = " | ".join(legend_group_parts) if legend_group_parts else None
            legend_group_title = None
            if legend_group and legend_group not in seen_legend_groups:
                legend_group_title = legend_group
                seen_legend_groups.add(legend_group)

            name_parts: list[str] = []
            if station_label is not None:
                name_parts.append(str(station_label))
            include_crop_in_name = crop_label is not None and crop_col not in category_cols
            if include_crop_in_name:
                name_parts.append(str(crop_label))

            show_year_in_name = year_label is not None and year_col not in category_cols
            prefix = " | ".join(name_parts)
            if show_year_in_name:
                prefix = f"{prefix} | {year_label}" if prefix else str(year_label)
            if not prefix:
                prefix = "All"

            label = (
                f"{prefix} (n={metrics.n}, r={metrics.r:.3f}, ubRMSE={metrics.ubrmse:.4f}, bias={metrics.bias:+.4f})"
            )
            fig.add_trace(
                go.Scatter(
                    x=group[target],
                    y=group[pred_col],
                    mode="markers",
                    marker=dict(color=palette[idx], size=7, opacity=0.85),
                    name=label,
                    legendgroup=legend_group,
                    legendgrouptitle=(
                        dict(text=_bold_text(legend_group_title), font=LEGEND_FONT)
                        if legend_group_title
                        else None
                    ),
                )
            )
        # 1:1 reference line
    fig.add_trace(
        go.Scatter(
            x=[xy_min, xy_max],
            y=[xy_min, xy_max],
            mode="lines",
            line=dict(color="#7f7f7f", dash="dash"),
            name="1:1",
        )
    )

    # Global regression fit
    overall_metrics = _compute_metrics(subset[target], subset[pred_col])
    if not np.isnan(overall_metrics.slope):
        fit_x = np.linspace(xy_min, xy_max, 50)
        fit_y = overall_metrics.slope * fit_x + overall_metrics.intercept
        fig.add_trace(
            go.Scatter(
                x=fit_x,
                y=fit_y,
                mode="lines",
                line=dict(color="#1f6feb", width=3),
                name=f"Regression (y = {overall_metrics.slope:.3f}x + {overall_metrics.intercept:.3f})",
            )
        )

    fig.update_layout(
        title=(
            dict(text=_bold_text(title_text), font=TITLE_FONT, x=0.5, xanchor="center")
            if title_text
            else None
        ),
        template="plotly_white",
        legend=dict(
            orientation="v",
            yanchor="top",
            y=1.0,
            x=1.02,
            xanchor="left",
            traceorder="grouped",
            tracegroupgap=4,
            itemsizing="constant",
            font=LEGEND_FONT,
            title=dict(font=LEGEND_FONT),
        ),
        margin=dict(l=70, r=320, t=70, b=70),
        width=1200,
        height=720,
        font=dict(size=TICK_FONT_SIZE, family=FONT_FAMILY_BASE),
    )

    fig.update_xaxes(
        range=[xy_min, xy_max],
        constrain="domain",
        title=dict(text=_bold_text("Observed SSM (m³/m³)"), font=AXIS_TITLE_FONT),
        tickfont=TICK_FONT,
    )
    fig.update_yaxes(
        range=[xy_min, xy_max],
        scaleanchor="x",
        scaleratio=1,
        title=dict(text=_bold_text("Predicted SSM (m³/m³)"), font=AXIS_TITLE_FONT),
        tickfont=TICK_FONT,
    )

    _ensure_vertical_legend_fit(fig)

    if save:
        filename = save if isinstance(save, str) else None
        export_width = save_width if save_width is not None else getattr(fig.layout, "width", None)
        if export_width is None:
            export_width = 1200
        export_height = save_height if save_height is not None else getattr(fig.layout, "height", None)
        if export_height is None:
            export_height = 720
        _save_figure(
            fig,
            filename,
            "scatter",
            workspace_dir=workspace_dir,
            width=export_width,
            height=export_height,
        )

    if save_html:
        html_name: Optional[str] = None
        include_js = "cdn"
        html_full = True
        html_config = None
        default_width = None
        default_height = None

        if isinstance(save_html, dict):
            html_name = save_html.get("name") or save_html.get("filename")
            include_js = save_html.get("include_plotlyjs", include_js)
            html_full = save_html.get("full_html", html_full)
            html_config = save_html.get("config", html_config)
            default_width = save_html.get("default_width")
            default_height = save_html.get("default_height")
        elif isinstance(save_html, str):
            html_name = save_html

        _save_figure_html(
            fig,
            html_name,
            "scatter",
            workspace_dir=workspace_dir,
            include_plotlyjs=include_js,
            full_html=html_full,
            config=html_config,
            default_width=default_width,
            default_height=default_height,
        )

    return fig


def plot_timeseries(
    risma_df: pd.DataFrame,
    holdout_df: pd.DataFrame,
    *,
    station: Optional[Iterable[str] | str] = None,
    year: Optional[Iterable[int] | int] = None,
    crop: Optional[Iterable[str] | str] = None,
    target: str = "ssm",
    target_label: Optional[str] = None,
    save: Union[bool, str] = False,
    save_html: Union[bool, str, dict] = False,
    save_width: Optional[int] = None,
    save_height: Optional[int] = None,
    workspace_dir: Optional[Union[str, Path]] = None,
    station_col: str = "station",
    year_col: str = "year",
    crop_col: str = "lc",
    date_col: str = "date",
    precip_col: str = "prcp",
    sst_col: str = "sst",
    freeze_threshold: float = 0.0,
    share_year: bool = False,
    title_text: Optional[str] = None,
    precip_opacity: float = 0.6,
    separate_rows: Optional[bool] = None,
) -> go.Figure:
    """Create a time-series validation view combining RISMA and predictions.

    Parameters are flexible enough to support different column naming
    conventions (defaults reflect the lowercase schema used in nitropulse).
    ``target_label`` lets you override the figure axis text while keeping the
    actual column name intact. ``share_year`` controls whether each year gets
    its own subplot, bringing all stations/crops for the same period together.
    ``precip_opacity`` controls the alpha of precipitation bars (default 0.6).
    """

    if separate_rows is not None:
        warnings.warn(
            "'separate_rows' is deprecated; use 'share_year' instead (note the inverted semantics).",
            FutureWarning,
            stacklevel=2,
        )
        share_year = not separate_rows

    holdout_subset = build_holdout_subset(
        holdout_df,
        station=station,
        year=year,
        crop=crop,
        target=target,
        station_col=station_col,
        year_col=year_col,
        crop_col=crop_col,
    )

    stations_available = sorted(holdout_subset[station_col].dropna().unique()) if station_col in holdout_subset else []
    if not stations_available:
        raise ValueError("No station data available for the selected filters.")

    obs_subset = risma_df.copy()
    obs_subset = _apply_filter(obs_subset, station_col, stations_available)
    obs_subset = _apply_filter(obs_subset, year_col, _as_list(year))
    obs_subset = _apply_filter(obs_subset, crop_col, crop)

    if obs_subset.empty:
        raise ValueError("No RISMA data available for the selected filters.")

    obs_subset[date_col] = pd.to_datetime(obs_subset[date_col])
    holdout_subset[date_col] = pd.to_datetime(holdout_subset[date_col])

    display_target = (target_label or target).replace('_', ' ').upper()

    def _normalize_key(value):
        if value is None or pd.isna(value):
            return None
        if isinstance(value, (np.integer, int)):
            return int(value)
        if isinstance(value, (np.floating, float)):
            if np.isnan(value):
                return None
            if float(value).is_integer():
                return int(value)
            return float(value)
        return str(value)

    combo_columns = []
    if crop_col in holdout_subset.columns or crop_col in obs_subset.columns:
        combo_columns.append(crop_col)
    if year_col in holdout_subset.columns or year_col in obs_subset.columns:
        combo_columns.append(year_col)

    share_year_enabled = share_year and (year_col in combo_columns)

    combos: set[tuple] = set()
    color_key_columns: tuple[str, ...] = ()

    def _collect_keys(columns: tuple[str, ...], *, require: tuple[str, ...] | None = None) -> list[tuple]:
        frames: list[pd.DataFrame] = []
        for df in (holdout_subset, obs_subset):
            if all(col in df.columns for col in columns):
                frames.append(df[list(columns)].copy())
        if not frames:
            return []
        merged = pd.concat(frames, ignore_index=True)
        keys: list[tuple] = []
        required_cols = require or columns
        for _, row in merged.iterrows():
            normalized = []
            valid = True
            for col in columns:
                value = _normalize_key(row[col] if col in row else None)
                normalized.append(value)
            for col, value in zip(columns, normalized):
                if col in required_cols and value is None:
                    valid = False
                    break
            if valid:
                keys.append(tuple(normalized))
        return list(dict.fromkeys(keys))

    priority = [
        ( (year_col, crop_col, station_col), (year_col, station_col) ),
        ( (year_col, station_col), (year_col, station_col) ),
        ( (crop_col, station_col), (station_col,) ),
        ( (year_col, crop_col), (year_col,) ),
        ( (station_col,), (station_col,) ),
    ]

    for columns, required in priority:
        keys = _collect_keys(columns, require=required)
        if keys:
            color_key_columns = columns
            combos.update(keys)
            break

    if not combos:
        combos.add((_normalize_key(stations_available[0]), None))
        color_key_columns = (station_col,)

    combos = sorted(combos, key=lambda key: tuple(str(part) for part in key))

    palette_global = _get_palette(len(combos))
    combo_to_color = {combo: palette_global[idx % len(palette_global)] for idx, combo in enumerate(combos)}
    symbols = [
        "circle",
        "square",
        "diamond",
        "cross",
        "x",
        "triangle-up",
        "triangle-down",
    ]

    if combo_columns:
        def _extract(df):
            cols = [c for c in combo_columns if c in df.columns]
            if not cols:
                return pd.DataFrame(columns=combo_columns)
            out = df[cols].copy()
            for missing in set(combo_columns) - set(cols):
                out[missing] = None
            return out

        unique_combos = _extract(holdout_subset)
        if unique_combos.empty:
            unique_combos = _extract(obs_subset)
        if unique_combos.empty:
            unique_combos = pd.DataFrame({col: [None] for col in combo_columns})
        unique_combos = unique_combos.drop_duplicates().sort_values(combo_columns).reset_index(drop=True)

        if share_year_enabled:
            year_values = []
            if year_col in unique_combos.columns:
                year_values = [
                    yr for yr in unique_combos[year_col].dropna().unique().tolist()
                ]
            if not year_values and year_col in obs_subset.columns:
                year_values = obs_subset[year_col].dropna().unique().tolist()
            if not year_values and year_col in holdout_subset.columns:
                year_values = holdout_subset[year_col].dropna().unique().tolist()

            year_values = sorted(year_values)
            if not year_values:
                year_values = [None]

            data: dict[str, list] = {year_col: year_values}
            if crop_col in combo_columns:
                data[crop_col] = [None] * len(year_values)

            unique_combos = pd.DataFrame(data)
            column_order = [col for col in (crop_col, year_col) if col in unique_combos.columns]
            if column_order:
                unique_combos = unique_combos[column_order]
    else:
        unique_combos = pd.DataFrame({crop_col: [None], year_col: [None]})

    stack_crops_by_year = crop_col in color_key_columns

    row_labels: list[str] = []
    multirow = share_year_enabled
    if share_year_enabled:
        for combo in unique_combos.itertuples(index=False):
            year_val = getattr(combo, year_col, None)
            if pd.isna(year_val):
                row_labels.append("All years")
            else:
                row_labels.append(f"Year {int(year_val) if isinstance(year_val, (int, np.integer)) else year_val}")

    if multirow:
        if row_labels:
            row_title_display = ["" for _ in row_labels]
        else:
            row_title_display = ["" for _ in unique_combos.index]
        fig = make_subplots(
            rows=len(unique_combos),
            cols=1,
            shared_xaxes=False,
            specs=[[{"secondary_y": True}] for _ in unique_combos.index],
            row_titles=row_title_display,
            vertical_spacing=0.05 if len(unique_combos) > 1 else 0.08,
        )
    else:
        fig = make_subplots(rows=1, cols=1, shared_xaxes=False, specs=[[{"secondary_y": True}]])

    seen_legends = set()
    legend_titles_seen: set[str] = set()
    precip_added = False
    row_ranges: dict[int, list[pd.Timestamp]] = {}

    x_axis_label = "Date"
    y_axis_label_left = "Soil Moisture (m³/m³)"
    y_axis_label_right = "Precipitation (mm/day)"

    for row_idx, combo in enumerate(unique_combos.itertuples(index=False), start=1):
        crop_val = getattr(combo, crop_col, None)
        year_val = getattr(combo, year_col, None)
        if share_year_enabled and crop_col in color_key_columns:
            crop_val = None
        row = row_idx if multirow else 1
        existing_range = row_ranges.get(row)
        if existing_range:
            row_min_date, row_max_date = existing_range
        else:
            row_min_date = None
            row_max_date = None
        if share_year_enabled:
            combo_label = row_labels[row_idx - 1] if row_labels else None
        else:
            label_parts = [str(v) for v in (crop_val, year_val) if v is not None]
            combo_label = " | ".join(label_parts) if label_parts else None

        hold_combo = holdout_subset.copy()
        obs_combo = obs_subset.copy()
        if crop_val is not None:
            if crop_col in hold_combo.columns:
                hold_combo = hold_combo[hold_combo[crop_col] == crop_val]
            if crop_col in obs_combo.columns:
                obs_combo = obs_combo[obs_combo[crop_col] == crop_val]
        if year_val is not None:
            if year_col in hold_combo.columns:
                hold_combo = hold_combo[hold_combo[year_col] == year_val]
            if year_col in obs_combo.columns:
                obs_combo = obs_combo[obs_combo[year_col] == year_val]

        if hold_combo.empty and obs_combo.empty:
            continue

        stations_available = sorted(hold_combo[station_col].dropna().unique()) if station_col in hold_combo else []
        if not stations_available:
            stations_available = sorted(obs_combo[station_col].dropna().unique()) if station_col in obs_combo else []

        for station_name in stations_available:
            obs_station = obs_combo[obs_combo[station_col] == station_name]
            hold_station = hold_combo[hold_combo[station_col] == station_name]
            if obs_station.empty:
                continue

            if share_year_enabled and year_val is not None:
                years_available = [year_val]
            else:
                years_available = (
                    sorted(hold_station[year_col].dropna().unique())
                    if year_col in hold_station
                    else []
                )
            if not years_available:
                years_available = sorted(obs_station[year_col].dropna().unique())

            for idx, yr in enumerate(years_available):
                if year_col in obs_station.columns and yr is not None:
                    obs_year = obs_station[obs_station[year_col] == yr]
                else:
                    obs_year = obs_station.copy()
                if obs_year.empty:
                    continue

                if year_col in hold_station.columns and yr is not None:
                    hold_year = hold_station[hold_station[year_col] == yr]
                else:
                    hold_year = hold_station.copy()

                if stack_crops_by_year:
                    crop_values: set = set()
                    if crop_col in obs_year.columns:
                        crop_values.update(obs_year[crop_col].dropna().unique().tolist())
                    if crop_col in hold_year.columns:
                        crop_values.update(hold_year[crop_col].dropna().unique().tolist())
                    crop_iter = sorted(crop_values, key=lambda v: str(v))
                    if not crop_iter:
                        crop_iter = [None]
                else:
                    crop_iter = [crop_val]

                for crop_idx, crop_value in enumerate(crop_iter):
                    obs_crop = obs_year
                    if crop_value is not None and crop_col in obs_crop.columns:
                        obs_crop = obs_crop[obs_crop[crop_col] == crop_value]
                    if obs_crop.empty:
                        continue

                    if crop_value is not None and crop_col in hold_year.columns:
                        hold_crop = hold_year[hold_year[crop_col] == crop_value]
                    else:
                        hold_crop = hold_year

                    norm_year = _normalize_key(yr)
                    norm_crop = _normalize_key(crop_value)
                    norm_station = _normalize_key(station_name)

                    mapping = {
                        year_col: norm_year,
                        crop_col: norm_crop,
                        station_col: norm_station,
                    }
                    color_key = tuple(mapping.get(col, None) for col in color_key_columns)
                    fallback_index = (idx + crop_idx) % len(palette_global)
                    color = combo_to_color.get(color_key, palette_global[fallback_index])

                    legend_key_obs = ("Observed", station_name, yr, crop_value)
                    legend_key_pred = ("Prediction", station_name, yr, crop_value)
                    showlegend_obs = True if multirow else (legend_key_obs not in seen_legends)
                    showlegend_pred = True if multirow else (legend_key_pred not in seen_legends)

                    if showlegend_obs and not multirow:
                        seen_legends.add(legend_key_obs)

                    legend_group_value_obs = combo_label or "Observed"
                    legend_title_obs = None
                    if showlegend_obs and combo_label and combo_label not in legend_titles_seen:
                        legend_titles_seen.add(combo_label)
                        legend_title_obs = combo_label

                    crop_suffix = ""
                    crop_display_str: Optional[str] = None
                    if crop_value is not None and not pd.isna(crop_value):
                        crop_display = int(crop_value) if isinstance(crop_value, (int, np.integer)) else crop_value
                        crop_display_str = str(crop_display)
                        crop_suffix = f" ({crop_display_str})"

                    if not obs_crop.empty and date_col in obs_crop.columns:
                        crop_min = obs_crop[date_col].min()
                        crop_max = obs_crop[date_col].max()
                        if pd.notna(crop_min):
                            row_min_date = crop_min if row_min_date is None else min(row_min_date, crop_min)
                        if pd.notna(crop_max):
                            row_max_date = crop_max if row_max_date is None else max(row_max_date, crop_max)

                    if combo_label:
                        name_obs = f"Observed {station_name}{crop_suffix}"
                    else:
                        suffix_parts: list[str] = []
                        if yr is not None and not pd.isna(yr):
                            yr_display = int(yr) if isinstance(yr, (int, np.integer)) else yr
                            suffix_parts.append(str(yr_display))
                        if crop_display_str is not None:
                            suffix_parts.append(crop_display_str)
                        suffix = f" ({', '.join(part for part in suffix_parts if part)})" if suffix_parts else ""
                        name_obs = f"Observed {station_name}{suffix}"

                    trace_obs = go.Scatter(
                        x=obs_crop[date_col],
                        y=obs_crop[target],
                        mode="lines",
                        line=dict(color=color, width=2),
                        name=name_obs,
                        showlegend=showlegend_obs,
                        legendgroup=legend_group_value_obs,
                        legendgrouptitle=(
                            dict(text=_bold_text(legend_title_obs), font=LEGEND_FONT)
                            if legend_title_obs
                            else None
                        ),
                    )
                    fig.add_trace(trace_obs, row=row, col=1, secondary_y=False)

                    if not hold_crop.empty:
                        if showlegend_pred and not multirow:
                            seen_legends.add(legend_key_pred)
                        legend_group_value_pred = combo_label or "Prediction"
                        legend_title_pred = None
                        if showlegend_pred and combo_label and combo_label not in legend_titles_seen:
                            legend_titles_seen.add(combo_label)
                            legend_title_pred = combo_label

                        if combo_label:
                            name_pred = f"Prediction {station_name}{crop_suffix}"
                        else:
                            suffix_parts_pred: list[str] = []
                            if yr is not None and not pd.isna(yr):
                                yr_display = int(yr) if isinstance(yr, (int, np.integer)) else yr
                                suffix_parts_pred.append(str(yr_display))
                            if crop_display_str is not None:
                                suffix_parts_pred.append(crop_display_str)
                            suffix_pred = (
                                f" ({', '.join(part for part in suffix_parts_pred if part)})"
                                if suffix_parts_pred
                                else ""
                            )
                            name_pred = f"Prediction {station_name}{suffix_pred}"

                        trace_pred = go.Scatter(
                            x=hold_crop[date_col],
                            y=hold_crop[f"{target}_pred"],
                            mode="markers",
                            marker=dict(
                                color=color,
                                size=7,
                                symbol=symbols[(idx + crop_idx) % len(symbols)],
                                line=dict(width=1, color="#1f1f20"),
                            ),
                            name=name_pred,
                            showlegend=showlegend_pred,
                            legendgroup=legend_group_value_pred,
                                legendgrouptitle=(
                                    dict(text=_bold_text(legend_title_pred), font=LEGEND_FONT)
                                    if legend_title_pred
                                    else None
                                ),
                        )
                        fig.add_trace(trace_pred, row=row, col=1, secondary_y=False)

                        if date_col in hold_crop.columns:
                            hold_min = hold_crop[date_col].min()
                            hold_max = hold_crop[date_col].max()
                            if pd.notna(hold_min):
                                row_min_date = hold_min if row_min_date is None else min(row_min_date, hold_min)
                            if pd.notna(hold_max):
                                row_max_date = hold_max if row_max_date is None else max(row_max_date, hold_max)

                    if (
                        precip_col in obs_crop.columns
                        and date_col in obs_crop.columns
                        and not obs_crop[precip_col].isna().all()
                    ):
                        prcp_daily_crop = (
                            obs_crop[[date_col, precip_col]]
                            .set_index(date_col)
                            .resample('D', label='left', closed='left')
                            .sum(min_count=1)
                            .fillna(0)
                        )
                        if (
                            not prcp_daily_crop.empty
                            and prcp_daily_crop[precip_col].abs().sum() > 0
                        ):
                            precip_label = str(station_name) if pd.notna(station_name) else "Station"
                            if not combo_label and pd.notna(yr):
                                precip_label = f"{precip_label} ({yr})"

                            legend_group_value_precip = combo_label or "Precipitation"
                            legend_title_precip = None
                            if combo_label and combo_label not in legend_titles_seen:
                                legend_titles_seen.add(combo_label)
                                legend_title_precip = combo_label

                            trace_precip = go.Bar(
                                x=prcp_daily_crop.index,
                                y=prcp_daily_crop[precip_col].values,
                                name=f"Precipitation {precip_label}",
                                marker_color=color,
                                opacity=precip_opacity,
                                showlegend=True,
                                legendgroup=legend_group_value_precip,
                                legendgrouptitle=(
                                    dict(text=_bold_text(legend_title_precip), font=LEGEND_FONT)
                                    if legend_title_precip
                                    else None
                                ),
                            )
                            fig.add_trace(trace_precip, row=row, col=1, secondary_y=True)
                            precip_added = True

        if row_min_date is not None and row_max_date is not None:
            row_ranges[row] = [row_min_date, row_max_date]

        # Freeze shading aggregated per crop-year
        freeze_spans = _freeze_spans_from_soil_temperature(
            obs_combo,
            date_col=date_col,
            temp_col=sst_col,
            threshold=freeze_threshold,
        )
        for start, end in freeze_spans:
            fig.add_vrect(
                x0=start,
                x1=end,
                fillcolor="rgba(150, 200, 255, 0.25)",
                layer="below",
                line_width=0,
                row=row,
                col=1,
            )

    row_count = len(unique_combos) if multirow else 1

    legend_config = dict(
        orientation="v",
        yanchor="top",
        y=1.0,
        x=1.02,
        xanchor="left",
        traceorder="grouped",
        tracegroupgap=4,
        itemsizing="constant",
        font=dict(size=LABEL_FONT_SIZE, family=FONT_FAMILY_BOLD),
        title=dict(font=dict(size=LABEL_FONT_SIZE, family=FONT_FAMILY_BOLD)),
    )

    def _legend_size() -> tuple[int, int]:
        entry_count = sum(
            1 for tr in fig.data if getattr(tr, "showlegend", False)
        )
        if entry_count == 0:
            entry_count = 1
        approx_line_height = legend_config["font"]["size"] + 6
        height_px = 20 + approx_line_height * entry_count
        width_px = 200
        return height_px, width_px

    legend_height, legend_width = _legend_size()

    if multirow:
        margin = dict(l=110, r=max(240, 60 + legend_width // 2), t=120, b=130)
        height = max(280 * row_count + 170, legend_height + 100)
        yaxis_config = dict(title=None, tickfont=TICK_FONT)
    else:
        margin = dict(l=60, r=max(240, legend_width + 80), t=80, b=60)
        height = max(360 * row_count, legend_height + 120)
        yaxis_config = dict(
            title=(
                dict(text=_bold_text(display_target), font=AXIS_TITLE_FONT)
                if display_target
                else None
            ),
            tickfont=TICK_FONT,
        )

    layout_title = (
        dict(text=_bold_text(title_text), font=TITLE_FONT, x=0.5, xanchor="center")
        if title_text
        else None
    )

    fig.update_layout(
        template="plotly_white",
        title=layout_title,
        yaxis=yaxis_config,
        legend=legend_config,
        margin=margin,
        height=height,
        font=dict(size=TICK_FONT_SIZE, family=FONT_FAMILY_BASE),
    )

    if multirow:
        for idx in range(1, row_count + 1):
            fig.update_yaxes(
                title=dict(text=_bold_text(y_axis_label_left), font=AXIS_TITLE_FONT),
                tickfont=TICK_FONT,
                row=idx,
                col=1,
            )
            fig.update_yaxes(
                title=dict(text=_bold_text(y_axis_label_right), font=AXIS_TITLE_FONT),
                tickfont=TICK_FONT,
                row=idx,
                col=1,
                secondary_y=True,
            )
        for idx in range(1, row_count + 1):
            fig.update_xaxes(
                title=dict(text=_bold_text(x_axis_label), font=AXIS_TITLE_FONT),
                tickfont=TICK_FONT,
                tickformat=DATE_TICKFORMAT,
                rangeslider=dict(visible=False),
                ticklabelmode="period",
                range=row_ranges.get(idx),
                row=idx,
                col=1,
            )
    else:
        fig.update_xaxes(
            title=dict(text=_bold_text("Date"), font=AXIS_TITLE_FONT),
            tickfont=TICK_FONT,
            tickformat=DATE_TICKFORMAT,
            rangeslider=dict(visible=False),
            ticklabelmode="period",
            range=row_ranges.get(1),
        )
        if not precip_added and precip_col in obs_subset:
            agg_precip = (
                obs_subset[[date_col, precip_col]]
                .set_index(date_col)
                .resample('D', label='left', closed='left')
                .sum(min_count=1)
                .fillna(0)
            )
            if not agg_precip.empty and agg_precip[precip_col].abs().sum() > 0:
                fig.add_trace(
                    go.Bar(
                        x=agg_precip.index,
                        y=agg_precip.values.flatten(),
                        name="Precipitation",
                        marker_color="rgba(255,105,180,1)",
                        opacity=precip_opacity,
                        showlegend=True,
                        legendgroup="Precipitation",
                    ),
                    secondary_y=True,
                )
        fig.update_yaxes(
            title=dict(text=_bold_text("Precipitation (mm/day)"), font=AXIS_TITLE_FONT),
            tickfont=TICK_FONT,
            secondary_y=True,
        )
        fig.update_yaxes(
            title=dict(text=_bold_text("Soil Moisture (m³/m³)"), font=AXIS_TITLE_FONT),
            tickfont=TICK_FONT,
            secondary_y=False,
        )

    _ensure_vertical_legend_fit(fig)

    if save:
        filename = save if isinstance(save, str) else None
        export_width = save_width if save_width is not None else getattr(fig.layout, "width", None)
        if export_width is None:
            export_width = 1600
        export_height = save_height if save_height is not None else getattr(fig.layout, "height", None)
        if export_height is None:
            export_height = height
        _save_figure(
            fig,
            filename,
            "timeseries",
            workspace_dir=workspace_dir,
            width=export_width,
            height=export_height,
        )

    if save_html:
        html_name: Optional[str] = None
        include_js = "cdn"
        html_full = True
        html_config = None
        default_width = None
        default_height = None

        if isinstance(save_html, dict):
            html_name = save_html.get("name") or save_html.get("filename")
            include_js = save_html.get("include_plotlyjs", include_js)
            html_full = save_html.get("full_html", html_full)
            html_config = save_html.get("config", html_config)
            default_width = save_html.get("default_width")
            default_height = save_html.get("default_height")
        elif isinstance(save_html, str):
            html_name = save_html
        _save_figure_html(
            fig,
            html_name,
            "timeseries",
            workspace_dir=workspace_dir,
            include_plotlyjs=include_js,
            full_html=html_full,
            config=html_config,
            default_width=default_width,
            default_height=default_height,
        )

    return fig


def create_scatter_plot(
    obs_df: pd.DataFrame,
    test_df: pd.DataFrame,
    *,
    station: Optional[Iterable[str] | str] = None,
    year: Optional[Iterable[int] | int] = None,
    crop: Optional[Iterable[str] | str] = None,
    target: str = "ssm",
    title_text: Optional[str] = None,
    save: Union[bool, str] = False,
    save_html: Union[bool, str, dict] = False,
    save_width: Optional[int] = None,
    save_height: Optional[int] = None,
    workspace_dir: Optional[Union[str, Path]] = None,
    **kwargs,
) -> go.Figure:
    """Convenience wrapper for :func:`plot_scatter` with optional filters."""

    return plot_scatter(
        obs_df, test_df,
        station=station,
        year=year,
        crop=crop,
        target=target,
        title_text=title_text,
        save=save,
        save_html=save_html,
        save_width=save_width,
        save_height=save_height,
        workspace_dir=workspace_dir,
        **kwargs,
    )


def create_timeseries_plot(
    risma_df: pd.DataFrame,
    test_df: pd.DataFrame,
    *,
    station: Optional[Iterable[str] | str] = None,
    year: Optional[Iterable[int] | int] = None,
    crop: Optional[Iterable[str] | str] = None,
    target: str = "ssm",
    target_label: Optional[str] = None,
    save: Union[bool, str] = False,
    save_html: Union[bool, str] = False,
    save_width: Optional[int] = None,
    save_height: Optional[int] = None,
    workspace_dir: Optional[Union[str, Path]] = None,
    **kwargs,
) -> go.Figure:
    """Convenience wrapper for :func:`plot_timeseries` with filtering.

    Accepts the same keyword arguments, including ``target_label`` for custom
    axis text. ``share_year=True`` mirrors the notebook behaviour where each
    year is rendered on its own subplot.
    """

    if "separate_rows" in kwargs:
        separate_rows = kwargs.pop("separate_rows")
        warnings.warn(
            "'separate_rows' is deprecated; use 'share_year' instead (note the inverted semantics).",
            FutureWarning,
            stacklevel=2,
        )
        kwargs.setdefault("share_year", not separate_rows)

    return plot_timeseries(
        risma_df,
        test_df,
        station=station,
        year=year,
        crop=crop,
        target=target,
        target_label=target_label,
        save=save,
        save_html=save_html,
        save_width=save_width,
        save_height=save_height,
        workspace_dir=workspace_dir,
        **kwargs,
    )


def _contiguous_date_ranges(dates: pd.Series, mask: pd.Series) -> Iterable[Tuple[pd.Timestamp, pd.Timestamp]]:
    """Collapse boolean mask into contiguous [start, end] intervals."""

    active = dates[mask].sort_values()
    if active.empty:
        return []

    spans = []
    start = active.iloc[0]
    prev = start
    for current in active.iloc[1:]:
        if (current - prev) > pd.Timedelta(days=1):
            spans.append((start, prev))
            start = current
        prev = current
    spans.append((start, prev))
    return spans


def _freeze_spans_from_soil_temperature(
    df: pd.DataFrame,
    *,
    date_col: str,
    temp_col: str,
    threshold: float,
) -> Iterable[Tuple[pd.Timestamp, pd.Timestamp]]:
    """Return contiguous frozen-soil spans derived solely from soil temperature."""

    if df.empty or date_col not in df or temp_col not in df:
        return []

    working = df[[date_col, temp_col]].dropna()
    if working.empty:
        return []

    # Aggregate to daily minima so a single below-threshold reading flags the day.
    daily_temp = (
        working.sort_values(date_col)
        .set_index(date_col)[temp_col]
        .resample('D', label='left', closed='left')
        .median()
    )
    if daily_temp.empty:
        return []

    freeze_mask = daily_temp <= threshold
    if not freeze_mask.any():
        return []

    date_series = pd.Series(daily_temp.index, index=daily_temp.index)
    return _contiguous_date_ranges(date_series, freeze_mask)


def summarize_crop_station_years(
    test_df: pd.DataFrame,
    *,
    station_col: str = "station",
    year_col: str = "year",
    crop_col: str = "lc",
) -> pd.DataFrame:
    """Return a pivot table of counts by station/year for each crop type."""

    required = [station_col, year_col, crop_col]
    missing = [col for col in required if col not in test_df.columns]
    if missing:
        raise KeyError(f"Required columns missing from dataframe: {missing}")

    working = test_df[required].dropna()
    if working.empty:
        raise ValueError("No data available after dropping rows with missing station/year/crop values.")

    working["_count"] = 1
    pivot = (
        working.pivot_table(
            index=[station_col, year_col],
            columns=crop_col,
            values="_count",
            aggfunc="count",
            fill_value=0,
        )
        .sort_index()
        .sort_index(axis=1)
    )
    pivot.index = pivot.index.set_names([station_col, year_col])
    pivot.columns.name = crop_col
    return pivot


def summarize_crop_coverage(
    test_df: pd.DataFrame,
    *,
    station_col: str = "station",
    year_col: str = "year",
    crop_col: str = "lc",
) -> pd.DataFrame:
    """Summarise availability of crops across stations and years."""

    required = [station_col, year_col, crop_col]
    missing = [col for col in required if col not in test_df.columns]
    if missing:
        raise KeyError(f"Required columns missing from dataframe: {missing}")

    df = test_df[required].dropna()
    if df.empty:
        raise ValueError("No data available after dropping rows with missing station/year/crop values.")

    unique_triplets = df.drop_duplicates([station_col, year_col, crop_col])

    records = df.groupby(crop_col).size()
    station_years = unique_triplets.groupby(crop_col).size()
    stations = unique_triplets.groupby(crop_col)[station_col].nunique()
    years = unique_triplets.groupby(crop_col)[year_col].nunique()

    summary = pd.DataFrame(
        {
            "records": records,
            "station_year_pairs": station_years,
            "unique_stations": stations,
            "unique_years": years,
        }
    ).fillna(0).astype(int).sort_index()

    return summary
