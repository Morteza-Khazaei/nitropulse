"""Plotting utilities for validating soil moisture estimates.

These helpers are designed to be imported inside notebooks. They accept
dataframes generated by the nitropulse pipeline (RISMA observations and
model holdout predictions) and emit Plotly figures mirroring the
validation visuals used during development.
"""

from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime
from typing import Iterable, Optional, Tuple, Union
import warnings

import numpy as np
import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots

from pathlib import Path

DATE_TICKFORMAT = "%b %Y"


def _as_list(value: Optional[Union[Iterable, str, int]]) -> Optional[list]:
    if value is None:
        return None
    if isinstance(value, (list, tuple, set, pd.Series, np.ndarray)):
        return list(value)
    return [value]


def _apply_filter(frame: pd.DataFrame, col: str, value) -> pd.DataFrame:
    values = _as_list(value)
    if values is None or col not in frame:
        return frame
    return frame[frame[col].isin(values)]


@dataclass
class Metrics:
    n: int
    r: float
    rmse: float
    ubrmse: float
    bias: float
    slope: float
    intercept: float


def build_holdout_subset(
    test_df: pd.DataFrame,
    station: Optional[Iterable[str] | str] = None,
    year: Optional[Iterable[int] | int] = None,
    *,
    crop: Optional[Iterable[str] | str] = None,
    target: str = "ssm",
    station_col: str = "station",
    year_col: str = "year",
    crop_col: str = "lc",
) -> pd.DataFrame:
    """Return a filtered view of ``test_df`` for the requested criteria.

    Parameters
    ----------
    test_df : pandas.DataFrame
        Holdout records produced by :class:`RegressionRF`. Must contain the
        columns specified below.
    station, year, crop : str or iterable, optional
        Values to filter by. ``None`` keeps all values for that dimension.
    target : str, default ``"ssm"``
        Name of the reference target column. A ``"<target>_pred"`` column is
        expected for the predictions.
    station_col, year_col, crop_col : str
        Column names that store the station and year information.
    """

    pred_col = f"{target}_pred"
    if target not in test_df or pred_col not in test_df:
        missing = {target, pred_col} - set(test_df.columns)
        raise KeyError(
            f"Holdout dataframe must contain {target!r} and {pred_col!r}; missing {missing}."
        )

    subset = test_df.copy()

    subset = _apply_filter(subset, station_col, station)
    subset = _apply_filter(subset, year_col, year)
    subset = _apply_filter(subset, crop_col, crop)

    if subset.empty:
        raise ValueError(
            "No records found for the specified station/year/crop filters."
        )
    sort_key = "doy" if "doy" in subset.columns else "date" if "date" in subset.columns else None
    if sort_key is not None:
        subset = subset.sort_values(sort_key)
    return subset


def _compute_metrics(obs: Iterable[float], pred: Iterable[float]) -> Metrics:
    obs_arr = np.asarray(list(obs), dtype=float)
    pred_arr = np.asarray(list(pred), dtype=float)
    mask = ~np.isnan(obs_arr) & ~np.isnan(pred_arr)
    obs_arr = obs_arr[mask]
    pred_arr = pred_arr[mask]
    if obs_arr.size == 0:
        raise ValueError("Cannot compute metrics with no valid pairs.")

    diff = obs_arr - pred_arr
    rmse = np.sqrt(np.mean(diff**2))
    bias = np.mean(pred_arr - obs_arr)
    ubrmse = np.sqrt(max(rmse**2 - bias**2, 0))

    if obs_arr.size < 2:
        r = np.nan
        slope = np.nan
        intercept = np.nan
    else:
        r = np.corrcoef(obs_arr, pred_arr)[0, 1]
        slope, intercept = np.polyfit(obs_arr, pred_arr, 1)

    return Metrics(
        n=obs_arr.size,
        r=r,
        rmse=rmse,
        ubrmse=ubrmse,
        bias=bias,
        slope=slope,
        intercept=intercept,
    )


def _format_metrics(metrics: Metrics) -> list[str]:
    return [
        f"n = {metrics.n}",
        f"r = {metrics.r:.3f}" if not np.isnan(metrics.r) else "r = n/a",
        f"RMSE = {metrics.rmse:.4f}",
        f"ubRMSE = {metrics.ubrmse:.4f}",
        f"Bias = {metrics.bias:+.4f}",
    ]


def _get_palette(n: int) -> list[str]:
    base = [
        "#1f77b4",
        "#ff7f0e",
        "#2ca02c",
        "#d62728",
        "#9467bd",
        "#8c564b",
        "#e377c2",
        "#7f7f7f",
        "#bcbd22",
        "#17becf",
    ]
    if n <= len(base):
        return base[:n]
    repeats = int(np.ceil(n / len(base)))
    return (base * repeats)[:n]


def _legend_entry_count(fig: go.Figure) -> int:
    """Count traces that will display in the legend."""

    count = 0
    for trace in fig.data:
        showlegend = getattr(trace, "showlegend", None)
        if showlegend is None:
            showlegend = True
        if showlegend:
            count += 1
    return count


def _ensure_vertical_legend_fit(
    fig: go.Figure,
    *,
    font_size: Optional[int] = None,
    width_estimate: int = 220,
    height_padding: int = 150,
    min_right_margin: int = 260,
    right_padding: int = 100,
) -> None:
    """Ensure vertical legends have enough canvas space, especially for export."""

    legend = getattr(fig.layout, "legend", None)
    if legend is None:
        return
    orientation = getattr(legend, "orientation", "v")
    if orientation != "v":
        return

    legend_font = getattr(legend, "font", None)
    font_size = font_size or (getattr(legend_font, "size", None) or 12)

    entry_count = max(1, _legend_entry_count(fig))
    approx_line_height = font_size + 6
    legend_height = 30 + approx_line_height * entry_count

    current_height = getattr(fig.layout, "height", None)
    margin = getattr(fig.layout, "margin", None)
    margin_l = getattr(margin, "l", None)
    margin_r = getattr(margin, "r", None)
    margin_t = getattr(margin, "t", None)
    margin_b = getattr(margin, "b", None)

    if margin_l is None:
        margin_l = 60
    if margin_r is None:
        margin_r = 60
    if margin_t is None:
        margin_t = 60
    if margin_b is None:
        margin_b = 60

    required_height = legend_height + height_padding + margin_t + margin_b
    if current_height is None or current_height < required_height:
        fig.update_layout(height=required_height)

    required_r = max(margin_r, min_right_margin, width_estimate + right_padding)

    fig.update_layout(
        margin=dict(l=margin_l, r=required_r, t=margin_t, b=margin_b)
    )


def _save_figure(
    fig: go.Figure,
    filename: Optional[str],
    prefix: str,
    *,
    workspace_dir: Optional[Union[str, Path]] = None,
    width: Optional[int] = None,
    height: Optional[int] = None,
) -> Optional[Path]:
    """Persist ``fig`` as a PNG within ``<workspace>/figures`` and return the path."""

    if fig is None:
        return None

    root = Path(workspace_dir).expanduser() if workspace_dir else (Path.home() / ".nitropulse")
    base_dir = root / "figures"
    base_dir.mkdir(parents=True, exist_ok=True)

    if filename:
        path = Path(filename)
        if not path.is_absolute():
            path = base_dir / path
    else:
        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        path = base_dir / f"{prefix}_{timestamp}.png"

    path = path.with_suffix(".png")

    write_kwargs = {}
    if width is not None:
        write_kwargs["width"] = width
    if height is not None:
        write_kwargs["height"] = height

    fig_to_save = go.Figure(fig)

    try:
        _ensure_date_tickformat(fig_to_save)
        _normalize_datetime_traces(fig_to_save)
        _ensure_vertical_legend_fit(fig_to_save)
        fig_to_save.write_image(str(path), **write_kwargs)
    except (ValueError, RuntimeError) as exc:  # pragma: no cover - runtime dependency
        message = str(exc)
        if "Chrome" in message or "kaleido" in message.lower() or "Browser" in message:
            warnings.warn(
                "Figure export skipped: Plotly's Kaleido engine (with Chrome dependencies) "
                "is not available. Install `kaleido` and run `plotly_get_chrome` to enable PNG saving.",
                RuntimeWarning,
            )
            return None
        warnings.warn(f"Figure export skipped due to unexpected error: {message}", RuntimeWarning)
        return None

    return path


def _ensure_date_tickformat(fig: go.Figure) -> None:
    axis_updates: dict[str, dict[str, str]] = {}
    for key in dir(fig.layout):
        if not key.startswith("xaxis"):
            continue
        axis = getattr(fig.layout, key, None)
        if axis is None:
            continue
        axis_type = getattr(axis, "type", None)
        tickformat = getattr(axis, "tickformat", None)
        if axis_type == "date":
            if not tickformat:
                axis_updates.setdefault(key, {})["tickformat"] = DATE_TICKFORMAT
        elif axis_type in (None, "-"):
            # Assume datetime-friendly axis if traces supplied timestamps.
            if _axis_has_datetime_data(fig, key):
                updates = axis_updates.setdefault(key, {})
                updates["type"] = "date"
                if not tickformat:
                    updates["tickformat"] = DATE_TICKFORMAT
    if axis_updates:
        fig.update_layout(**axis_updates)


def _normalize_datetime_traces(fig: go.Figure) -> None:
    for trace in fig.data:
        values = getattr(trace, "x", None)
        if not _sequence_is_datetime(values):
            continue
        normalized = _to_datetime_list(values)
        setattr(trace, "x", normalized)


def _axis_has_datetime_data(fig: go.Figure, axis_key: str) -> bool:
    axis_ref = "x" if axis_key == "xaxis" else axis_key.replace("xaxis", "x")
    for trace in fig.data:
        trace_axis = getattr(trace, "xaxis", None)
        if trace_axis and trace_axis != axis_ref:
            continue
        values = getattr(trace, "x", None)
        if _sequence_is_datetime(values):
            return True
    return False


def _sequence_is_datetime(values) -> bool:
    if values is None:
        return False
    if isinstance(values, (pd.Series, pd.Index)):
        return pd.api.types.is_datetime64_any_dtype(values)
    if isinstance(values, np.ndarray):
        if values.size == 0:
            return False
        if np.issubdtype(values.dtype, np.datetime64):
            return True
        values = values.tolist()
    if isinstance(values, (list, tuple)):
        for val in values:
            if val is None:
                continue
            if isinstance(val, (pd.Timestamp, np.datetime64, datetime)):
                return True
            if hasattr(val, "to_pydatetime"):
                try:
                    val.to_pydatetime()
                    return True
                except Exception:
                    continue
    return False


def _to_datetime_list(values) -> list:
    if isinstance(values, pd.Series):
        return [pd.to_datetime(v).to_pydatetime() if not pd.isna(v) else None for v in values]
    if isinstance(values, pd.Index):
        return [pd.to_datetime(v).to_pydatetime() if not pd.isna(v) else None for v in values]
    if isinstance(values, np.ndarray):
        values = values.tolist()
    normalized = []
    for val in values:
        if val is None:
            normalized.append(None)
            continue
        if isinstance(val, datetime):
            normalized.append(val)
            continue
        if isinstance(val, np.datetime64):
            normalized.append(pd.to_datetime(val).to_pydatetime())
            continue
        if isinstance(val, pd.Timestamp):
            normalized.append(val.to_pydatetime())
            continue
        if hasattr(val, "to_pydatetime"):
            try:
                normalized.append(val.to_pydatetime())
                continue
            except Exception:
                pass
        normalized.append(pd.to_datetime(val).to_pydatetime())
    return normalized


def plot_scatter(
    obs_df: pd.DataFrame,
    holdout_df: pd.DataFrame,
    *,
    station: Optional[Iterable[str] | str] = None,
    year: Optional[Iterable[int] | int] = None,
    crop: Optional[Iterable[str] | str] = None,
    target: str = "ssm",
    station_col: str = "station",
    year_col: str = "year",
    crop_col: str = "lc",
    group_cols: Optional[Iterable[str]] = None,
    save: Union[bool, str] = False,
    save_width: Optional[int] = None,
    save_height: Optional[int] = None,
    workspace_dir: Optional[Union[str, Path]] = None,
) -> go.Figure:
    """Create a scatter validation plot for any combination of filters.

    Automatically determines the grouping (colour/style) based on which of
    ``station``, ``year`` and ``crop`` still vary after filtering, unless
    ``group_cols`` is provided explicitly.
    """

    subset = build_holdout_subset(
        holdout_df,
        station=station,
        year=year,
        crop=crop,
        target=target,
        station_col=station_col,
        year_col=year_col,
        crop_col=crop_col,
    )

    if target not in subset.columns or subset[target].isna().all():
        if target not in obs_df.columns:
            raise KeyError(f"Target column '{target}' not found in observation dataframe.")
        obs_subset = build_holdout_subset(
            obs_df,
            station=station,
            year=year,
            crop=crop,
            target=target,
            station_col=station_col,
            year_col=year_col,
            crop_col=crop_col,
        )
        merge_keys = [col for col in [station_col, year_col, crop_col, 'doy', 'date'] if col in subset.columns and col in obs_subset.columns]
        if not merge_keys:
            raise ValueError("Unable to align observation and prediction dataframes; ensure shared keys such as station/year/doy or date are present.")
        obs_subset = obs_subset[merge_keys + [target]]
        subset = subset.merge(obs_subset, on=merge_keys, how='left', suffixes=('', '_obs'))
        obs_column = f"{target}_obs"
        if obs_column in subset.columns:
            subset[target] = subset[target].fillna(subset.pop(obs_column))

    pred_col = f"{target}_pred"

    if group_cols is None:
        group_cols = []
        potential = [
            (station_col, station),
            (year_col, year),
            (crop_col, crop),
        ]
        for col, value in potential:
            if col not in subset:
                continue
            unique_vals = subset[col].dropna().unique()
            values = _as_list(value)
            if values is None:
                if len(unique_vals) > 1:
                    group_cols.append(col)
            else:
                if len(set(unique_vals)) > 1:
                    group_cols.append(col)

    # Ensure group columns are unique and present
    group_cols = [c for c in dict.fromkeys(group_cols) if c in subset.columns]

    fig = go.Figure()
    seen_legend_groups: set[str] = set()

    # Determine plotting ranges across all points once
    xy_min = float(np.nanmin(subset[[target, pred_col]].values))
    xy_max = float(np.nanmax(subset[[target, pred_col]].values))
    if not np.isfinite(xy_min) or not np.isfinite(xy_max):
        raise ValueError("Cannot determine axis limits due to non-finite values.")
    if xy_min == xy_max:
        xy_min -= 0.01
        xy_max += 0.01
    else:
        pad = (xy_max - xy_min) * 0.05
        xy_min -= pad
        xy_max += pad

    if not group_cols:
        metrics = _compute_metrics(subset[target], subset[pred_col])
        trace_name = (
            f"station={station or 'all'} | lc={crop or 'all'} | year={year or 'all'} (n={metrics.n}, r={metrics.r:.3f}, ubRMSE={metrics.ubrmse:.4f}, bias={metrics.bias:+.4f})"
        )
        fig.add_trace(
            go.Scatter(
                x=subset[target],
                y=subset[pred_col],
                mode="markers",
                marker=dict(color="#1f1f20", size=7, opacity=0.9),
                name=trace_name,
            )
        )
    else:
        grouped = subset.groupby(group_cols, dropna=False)
        palette = _get_palette(len(grouped))
        for idx, (key, group) in enumerate(grouped):
            metrics = _compute_metrics(group[target], group[pred_col])
            if isinstance(key, tuple):
                key_map = {col: val for col, val in zip(group_cols, key)}
            else:
                key_map = {group_cols[0]: key}

            station_label = key_map.get(station_col)
            if station_label is None and station is not None:
                station_vals = _as_list(station)
                if station_vals and len(set(station_vals)) == 1:
                    station_label = station_vals[0]
            if station_label is None:
                station_label = "station"

            crop_label = key_map.get(crop_col)
            if crop_label is None and crop is not None:
                crop_vals = _as_list(crop)
                if crop_vals and len(set(crop_vals)) == 1:
                    crop_label = crop_vals[0]
            if crop_label is None:
                crop_label = "crop"

            year_label = key_map.get(year_col)
            if year_label is None and year is not None:
                year_vals = _as_list(year)
                if year_vals and len(set(year_vals)) == 1:
                    year_label = year_vals[0]
            if year_label is None:
                year_label = "year"
            legend_group_parts: list[str] = []
            legend_group_title = None
            if crop_label != "crop":
                legend_group_parts.append(str(crop_label))
            if year_label != "year":
                legend_group_parts.append(str(year_label))
            legend_group = " | ".join(legend_group_parts) if legend_group_parts else None
            if legend_group and legend_group not in seen_legend_groups:
                legend_group_title = legend_group
                seen_legend_groups.add(legend_group)

            name_parts: list[str] = []
            if station_label != "station":
                name_parts.append(str(station_label))
            if not legend_group:
                if crop_label != "crop":
                    name_parts.append(str(crop_label))
                if year_label != "year":
                    name_parts.append(str(year_label))
            prefix = " | ".join(name_parts) if name_parts else "All"
            label = (
                f"{prefix} (n={metrics.n}, r={metrics.r:.3f}, ubRMSE={metrics.ubrmse:.4f}, bias={metrics.bias:+.4f})"
            )
            fig.add_trace(
                go.Scatter(
                    x=group[target],
                    y=group[pred_col],
                    mode="markers",
                    marker=dict(color=palette[idx], size=7, opacity=0.85),
                    name=label,
                    legendgroup=legend_group,
                    legendgrouptitle=dict(text=legend_group_title) if legend_group_title else None,
                )
            )
        # 1:1 reference line
    fig.add_trace(
        go.Scatter(
            x=[xy_min, xy_max],
            y=[xy_min, xy_max],
            mode="lines",
            line=dict(color="#7f7f7f", dash="dash"),
            name="1:1",
        )
    )

    # Global regression fit
    overall_metrics = _compute_metrics(subset[target], subset[pred_col])
    if not np.isnan(overall_metrics.slope):
        fit_x = np.linspace(xy_min, xy_max, 50)
        fit_y = overall_metrics.slope * fit_x + overall_metrics.intercept
        fig.add_trace(
            go.Scatter(
                x=fit_x,
                y=fit_y,
                mode="lines",
                line=dict(color="#1f6feb", width=3),
                name=f"Regression (y = {overall_metrics.slope:.3f}x + {overall_metrics.intercept:.3f})",
            )
        )

    fig.update_layout(
        title=None,
        xaxis_title="Observed SSM (m³/m³)",
        yaxis_title="Predicted SSM (m³/m³)",
        template="plotly_white",
        legend=dict(
            orientation="v",
            yanchor="top",
            y=1.0,
            x=1.02,
            xanchor="left",
            traceorder="grouped",
            tracegroupgap=4,
            itemsizing="constant",
            font=dict(size=12),
        ),
        margin=dict(l=70, r=320, t=70, b=70),
        width=1200,
        height=720,
    )

    fig.update_xaxes(range=[xy_min, xy_max], constrain="domain")
    fig.update_yaxes(range=[xy_min, xy_max], scaleanchor="x", scaleratio=1)

    _ensure_vertical_legend_fit(fig)

    if save:
        filename = save if isinstance(save, str) else None
        export_width = save_width if save_width is not None else getattr(fig.layout, "width", None)
        if export_width is None:
            export_width = 1200
        export_height = save_height if save_height is not None else getattr(fig.layout, "height", None)
        if export_height is None:
            export_height = 720
        _save_figure(
            fig,
            filename,
            "scatter",
            workspace_dir=workspace_dir,
            width=export_width,
            height=export_height,
        )

    return fig


def plot_timeseries(
    risma_df: pd.DataFrame,
    holdout_df: pd.DataFrame,
    *,
    station: Optional[Iterable[str] | str] = None,
    year: Optional[Iterable[int] | int] = None,
    crop: Optional[Iterable[str] | str] = None,
    target: str = "ssm",
    target_label: Optional[str] = None,
    save: Union[bool, str] = False,
    save_width: Optional[int] = None,
    save_height: Optional[int] = None,
    workspace_dir: Optional[Union[str, Path]] = None,
    station_col: str = "station",
    year_col: str = "year",
    crop_col: str = "lc",
    date_col: str = "date",
    precip_col: str = "prcp",
    sst_col: str = "sst",
    freeze_threshold: float = 0.0,
    separate_rows: bool = False,
    title_text: Optional[str] = None,
    precip_opacity: float = 0.6,
) -> go.Figure:
    """Create a time-series validation view combining RISMA and predictions.

    Parameters are flexible enough to support different column naming
    conventions (defaults reflect the lowercase schema used in nitropulse).
    ``target_label`` lets you override the figure axis/title text while keeping
    the actual column name intact. ``precip_opacity`` controls the alpha of
    precipitation bars (default 0.6).
    """

    holdout_subset = build_holdout_subset(
        holdout_df,
        station=station,
        year=year,
        crop=crop,
        target=target,
        station_col=station_col,
        year_col=year_col,
        crop_col=crop_col,
    )

    stations_available = sorted(holdout_subset[station_col].dropna().unique()) if station_col in holdout_subset else []
    if not stations_available:
        raise ValueError("No station data available for the selected filters.")

    obs_subset = risma_df.copy()
    obs_subset = _apply_filter(obs_subset, station_col, stations_available)
    obs_subset = _apply_filter(obs_subset, year_col, _as_list(year))
    obs_subset = _apply_filter(obs_subset, crop_col, crop)

    if obs_subset.empty:
        raise ValueError("No RISMA data available for the selected filters.")

    obs_subset[date_col] = pd.to_datetime(obs_subset[date_col])
    holdout_subset[date_col] = pd.to_datetime(holdout_subset[date_col])

    combos_df = holdout_subset[[station_col, year_col]].dropna()
    if combos_df.empty:
        combos_df = obs_subset[[station_col, year_col]].dropna()
    combos = list(combos_df.drop_duplicates().apply(lambda r: (r[station_col], r[year_col]), axis=1))
    if not combos:
        combos = [(stations_available[0], None)]
    palette_global = _get_palette(len(combos))
    combo_to_color = {combo: palette_global[idx % len(palette_global)] for idx, combo in enumerate(combos)}
    symbols = [
        "circle",
        "square",
        "diamond",
        "cross",
        "x",
        "triangle-up",
        "triangle-down",
    ]

    display_target = (target_label or target).replace('_', ' ').upper()

    combo_columns = []
    if crop_col in holdout_subset.columns or crop_col in obs_subset.columns:
        combo_columns.append(crop_col)
    if year_col in holdout_subset.columns or year_col in obs_subset.columns:
        combo_columns.append(year_col)

    if combo_columns:
        def _extract(df):
            cols = [c for c in combo_columns if c in df.columns]
            if not cols:
                return pd.DataFrame(columns=combo_columns)
            out = df[cols].copy()
            for missing in set(combo_columns) - set(cols):
                out[missing] = None
            return out

        unique_combos = _extract(holdout_subset)
        if unique_combos.empty:
            unique_combos = _extract(obs_subset)
        if unique_combos.empty:
            unique_combos = pd.DataFrame({col: [None] for col in combo_columns})
        unique_combos = unique_combos.drop_duplicates().sort_values(combo_columns).reset_index(drop=True)
    else:
        unique_combos = pd.DataFrame({crop_col: [None], year_col: [None]})

    row_labels: list[str] = []
    if separate_rows:
        for combo in unique_combos.itertuples(index=False):
            crop_val = getattr(combo, crop_col, None)
            year_val = getattr(combo, year_col, None)
            label = " | ".join(
                [str(v) for v in (crop_val, year_val) if v is not None]
            ) or "All"
            row_labels.append(label)

        fig = make_subplots(
            rows=len(unique_combos),
            cols=1,
            shared_xaxes=False,
            specs=[[{"secondary_y": True}] for _ in unique_combos.index],
            row_titles=["" for _ in unique_combos.index],
            vertical_spacing=0.05 if len(unique_combos) > 1 else 0.08,
        )
    else:
        fig = make_subplots(rows=1, cols=1, shared_xaxes=False, specs=[[{"secondary_y": True}]])

    seen_legends = set()
    legend_titles_seen: set[str] = set()
    precip_added = False
    combined_precip_series: list[tuple[Optional[str], str, pd.Series]] = []

    x_axis_label = "Date"
    y_axis_label_left = "Soil Moisture (m³/m³)"
    y_axis_label_right = "Precipitation (mm/day)"

    for row_idx, combo in enumerate(unique_combos.itertuples(index=False), start=1):
        crop_val = getattr(combo, crop_col, None)
        year_val = getattr(combo, year_col, None)
        row = row_idx if separate_rows else 1
        if separate_rows:
            combo_label = row_labels[row_idx - 1]
        else:
            label_parts = [str(v) for v in (crop_val, year_val) if v is not None]
            combo_label = " | ".join(label_parts) if label_parts else None

        hold_combo = holdout_subset.copy()
        obs_combo = obs_subset.copy()
        if crop_val is not None:
            if crop_col in hold_combo.columns:
                hold_combo = hold_combo[hold_combo[crop_col] == crop_val]
            if crop_col in obs_combo.columns:
                obs_combo = obs_combo[obs_combo[crop_col] == crop_val]
        if year_val is not None:
            if year_col in hold_combo.columns:
                hold_combo = hold_combo[hold_combo[year_col] == year_val]
            if year_col in obs_combo.columns:
                obs_combo = obs_combo[obs_combo[year_col] == year_val]

        if hold_combo.empty and obs_combo.empty:
            continue

        stations_available = sorted(hold_combo[station_col].dropna().unique()) if station_col in hold_combo else []
        if not stations_available:
            stations_available = sorted(obs_combo[station_col].dropna().unique()) if station_col in obs_combo else []

        for station_name in stations_available:
            obs_station = obs_combo[obs_combo[station_col] == station_name]
            hold_station = hold_combo[hold_combo[station_col] == station_name]
            if obs_station.empty:
                continue

            years_available = sorted(hold_station[year_col].dropna().unique()) if year_col in hold_station else []
            if not years_available:
                years_available = sorted(obs_station[year_col].dropna().unique())

            for idx, yr in enumerate(years_available):
                obs_year = obs_station[obs_station[year_col] == yr]
                if obs_year.empty:
                    continue
                combo_key = (station_name, yr)
                color = combo_to_color.get(combo_key, palette_global[idx % len(palette_global)])

                legend_key_obs = ("Observed", station_name, yr)
                legend_key_pred = ("Prediction", station_name, yr)
                showlegend_obs = True if separate_rows else (legend_key_obs not in seen_legends)
                showlegend_pred = True if separate_rows else (legend_key_pred not in seen_legends)

                if showlegend_obs and not separate_rows:
                    seen_legends.add(legend_key_obs)

                legend_group_value_obs = combo_label or "Observed"
                legend_title_obs = None
                if showlegend_obs and combo_label and combo_label not in legend_titles_seen:
                    legend_titles_seen.add(combo_label)
                    legend_title_obs = combo_label

                if combo_label:
                    name_obs = f"Observed {station_name}"
                else:
                    suffix = f" ({yr})" if yr is not None else ""
                    name_obs = f"Observed {station_name}{suffix}"

                trace_obs = go.Scatter(
                    x=obs_year[date_col],
                    y=obs_year[target],
                    mode="lines",
                    line=dict(color=color, width=2),
                    name=name_obs,
                    showlegend=showlegend_obs,
                    legendgroup=legend_group_value_obs,
                    legendgrouptitle=dict(text=legend_title_obs) if legend_title_obs else None,
                )
                fig.add_trace(trace_obs, row=row, col=1, secondary_y=False)

                hold_year = hold_station[hold_station[year_col] == yr]
                if not hold_year.empty:
                    if showlegend_pred and not separate_rows:
                        seen_legends.add(legend_key_pred)
                    legend_group_value_pred = combo_label or "Prediction"
                    legend_title_pred = None
                    if showlegend_pred and combo_label and combo_label not in legend_titles_seen:
                        legend_titles_seen.add(combo_label)
                        legend_title_pred = combo_label

                    if combo_label:
                        name_pred = f"Prediction {station_name}"
                    else:
                        suffix = f" ({yr})" if yr is not None else ""
                        name_pred = f"Prediction {station_name}{suffix}"

                    trace_pred = go.Scatter(
                        x=hold_year[date_col],
                        y=hold_year[f"{target}_pred"],
                        mode="markers",
                        marker=dict(
                            color=color,
                            size=7,
                            symbol=symbols[idx % len(symbols)],
                            line=dict(width=1, color="#1f1f20"),
                        ),
                        name=name_pred,
                        showlegend=showlegend_pred,
                        legendgroup=legend_group_value_pred,
                        legendgrouptitle=dict(text=legend_title_pred) if legend_title_pred else None,
                    )
                    fig.add_trace(trace_pred, row=row, col=1, secondary_y=False)

                if (
                    precip_col in obs_year.columns
                    and date_col in obs_year.columns
                    and not obs_year[precip_col].isna().all()
                ):
                    prcp_daily_year = (
                        obs_year[[date_col, precip_col]]
                        .set_index(date_col)
                        .resample('D', label='left', closed='left')
                        .sum(min_count=1)
                        .fillna(0)
                    )
                    if (
                        not prcp_daily_year.empty
                        and prcp_daily_year[precip_col].abs().sum() > 0
                    ):
                        precip_label = str(station_name) if pd.notna(station_name) else "Station"
                        if not combo_label:
                            label_suffix = None
                            if pd.notna(yr):
                                label_suffix = yr
                            elif pd.notna(year_val):
                                label_suffix = year_val
                            elif pd.notna(crop_val):
                                label_suffix = crop_val
                            if label_suffix is not None:
                                precip_label = f"{precip_label} ({label_suffix})"

                        if separate_rows:
                            legend_group_value_precip = combo_label or "Precipitation"
                            legend_title_precip = None
                            if combo_label and combo_label not in legend_titles_seen:
                                legend_titles_seen.add(combo_label)
                                legend_title_precip = combo_label

                            trace_precip = go.Bar(
                                x=prcp_daily_year.index,
                                y=prcp_daily_year[precip_col].values,
                                name=f"Precipitation {precip_label}",
                                marker_color=color,
                                opacity=precip_opacity,
                                showlegend=True,
                                legendgroup=legend_group_value_precip,
                                legendgrouptitle=dict(text=legend_title_precip) if legend_title_precip else None,
                            )
                            fig.add_trace(trace_precip, row=row, col=1, secondary_y=True)
                            precip_added = True
                        else:
                            combined_precip_series.append((combo_label, precip_label, prcp_daily_year[precip_col]))
                            precip_added = True

        # Freeze shading aggregated per crop-year
        freeze_spans = _freeze_spans_from_soil_temperature(
            obs_combo,
            date_col=date_col,
            temp_col=sst_col,
            threshold=freeze_threshold,
        )
        for start, end in freeze_spans:
            fig.add_vrect(
                x0=start,
                x1=end,
                fillcolor="rgba(150, 200, 255, 0.25)",
                layer="below",
                line_width=0,
                row=row,
                col=1,
            )

    row_count = len(unique_combos) if separate_rows else 1

    legend_config = dict(
        orientation="v",
        yanchor="top",
        y=1.0,
        x=1.02,
        xanchor="left",
        traceorder="grouped",
        tracegroupgap=4,
        itemsizing="constant",
        font=dict(size=12),
    )

    def _legend_size() -> tuple[int, int]:
        entry_count = sum(
            1 for tr in fig.data if getattr(tr, "showlegend", False)
        )
        if entry_count == 0:
            entry_count = 1
        approx_line_height = legend_config["font"]["size"] + 6
        height_px = 20 + approx_line_height * entry_count
        width_px = 200
        return height_px, width_px

    legend_height, legend_width = _legend_size()

    if separate_rows:
        margin = dict(l=110, r=max(240, 60 + legend_width // 2), t=120, b=130)
        height = max(280 * row_count + 170, legend_height + 100)
        yaxis_config = dict(title=None)
    else:
        margin = dict(l=60, r=max(240, legend_width + 80), t=80, b=60)
        height = max(360 * row_count, legend_height + 120)
        yaxis_config = dict(title=display_target)

    fig.update_layout(
        template="plotly_white",
        title=None,
        yaxis=yaxis_config,
        legend=legend_config,
        margin=margin,
        height=height,
        font=dict(size=12),
    )

    if separate_rows:
        for idx in range(1, row_count + 1):
            fig.update_yaxes(
                title=dict(text=y_axis_label_left, font=dict(size=13)),
                tickfont=dict(size=11),
                row=idx,
                col=1,
            )
            fig.update_yaxes(
                title=dict(text=y_axis_label_right, font=dict(size=13)),
                tickfont=dict(size=11),
                row=idx,
                col=1,
                secondary_y=True,
            )
        for idx in range(1, row_count + 1):
            fig.update_xaxes(
                title=dict(text=x_axis_label, font=dict(size=13)),
                tickfont=dict(size=11),
                tickformat=DATE_TICKFORMAT,
                rangeslider=dict(visible=False),
                ticklabelmode="period",
                row=idx,
                col=1,
            )
    else:
        fig.update_xaxes(
            title=dict(text="Date", font=dict(size=13)),
            tickfont=dict(size=11),
            tickformat=DATE_TICKFORMAT,
            rangeslider=dict(visible=False),
            ticklabelmode="period",
        )
        if combined_precip_series:
            precip_palette = _get_palette(len(combined_precip_series))
            for idx, (group_label, st_name, series) in enumerate(combined_precip_series):
                legend_group_value = group_label or "Precipitation"
                legend_title_precip = None
                if group_label and group_label not in legend_titles_seen:
                    legend_titles_seen.add(group_label)
                    legend_title_precip = group_label
                fig.add_trace(
                    go.Bar(
                        x=series.index,
                        y=series.values,
                        name=f"Precipitation {st_name}",
                        marker_color=precip_palette[idx],
                        opacity=precip_opacity,
                        showlegend=True,
                        legendgroup=legend_group_value,
                        legendgrouptitle=dict(text=legend_title_precip) if legend_title_precip else None,
                    ),
                    secondary_y=True,
                )
        elif precip_col in obs_subset:
            agg_precip = (
                obs_subset[[date_col, precip_col]]
                .set_index(date_col)
                .resample('D', label='left', closed='left')
                .sum(min_count=1)
                .fillna(0)
            )
            if not agg_precip.empty and agg_precip[precip_col].abs().sum() > 0:
                fig.add_trace(
                    go.Bar(
                        x=agg_precip.index,
                        y=agg_precip.values.flatten(),
                        name="Precipitation",
                        marker_color="rgba(255,105,180,1)",
                        opacity=precip_opacity,
                        showlegend=True,
                        legendgroup="Precipitation",
                    ),
                    secondary_y=True,
                )
        fig.update_yaxes(
            title=dict(text="Precipitation (mm/day)", font=dict(size=13)),
            tickfont=dict(size=11),
            secondary_y=True,
        )
        fig.update_yaxes(
            title=dict(text="Soil Moisture (m³/m³)", font=dict(size=13)),
            tickfont=dict(size=11),
            secondary_y=False,
        )

    _ensure_vertical_legend_fit(fig)

    if save:
        filename = save if isinstance(save, str) else None
        export_width = save_width if save_width is not None else getattr(fig.layout, "width", None)
        if export_width is None:
            export_width = 1600
        export_height = save_height if save_height is not None else getattr(fig.layout, "height", None)
        if export_height is None:
            export_height = height
        _save_figure(
            fig,
            filename,
            "timeseries",
            workspace_dir=workspace_dir,
            width=export_width,
            height=export_height,
        )

    return fig


def create_scatter_plot(
    obs_df: pd.DataFrame,
    test_df: pd.DataFrame,
    *,
    station: Optional[Iterable[str] | str] = None,
    year: Optional[Iterable[int] | int] = None,
    crop: Optional[Iterable[str] | str] = None,
    target: str = "ssm",
    save: Union[bool, str] = False,
    save_width: Optional[int] = None,
    save_height: Optional[int] = None,
    workspace_dir: Optional[Union[str, Path]] = None,
    **kwargs,
) -> go.Figure:
    """Convenience wrapper for :func:`plot_scatter` with optional filters."""

    return plot_scatter(
        obs_df, test_df,
        station=station,
        year=year,
        crop=crop,
        target=target,
        save=save,
        save_width=save_width,
        save_height=save_height,
        workspace_dir=workspace_dir,
        **kwargs,
    )


def create_timeseries_plot(
    risma_df: pd.DataFrame,
    test_df: pd.DataFrame,
    *,
    station: Optional[Iterable[str] | str] = None,
    year: Optional[Iterable[int] | int] = None,
    crop: Optional[Iterable[str] | str] = None,
    target: str = "ssm",
    target_label: Optional[str] = None,
    save: Union[bool, str] = False,
    save_width: Optional[int] = None,
    save_height: Optional[int] = None,
    workspace_dir: Optional[Union[str, Path]] = None,
    **kwargs,
) -> go.Figure:
    """Convenience wrapper for :func:`plot_timeseries` with filtering.

    Accepts the same keyword arguments, including ``target_label`` for custom
    axis text.
    """

    return plot_timeseries(
        risma_df,
        test_df,
        station=station,
        year=year,
        crop=crop,
        target=target,
        target_label=target_label,
        save=save,
        save_width=save_width,
        save_height=save_height,
        workspace_dir=workspace_dir,
        **kwargs,
    )


def _contiguous_date_ranges(dates: pd.Series, mask: pd.Series) -> Iterable[Tuple[pd.Timestamp, pd.Timestamp]]:
    """Collapse boolean mask into contiguous [start, end] intervals."""

    active = dates[mask].sort_values()
    if active.empty:
        return []

    spans = []
    start = active.iloc[0]
    prev = start
    for current in active.iloc[1:]:
        if (current - prev) > pd.Timedelta(days=1):
            spans.append((start, prev))
            start = current
        prev = current
    spans.append((start, prev))
    return spans


def _freeze_spans_from_soil_temperature(
    df: pd.DataFrame,
    *,
    date_col: str,
    temp_col: str,
    threshold: float,
) -> Iterable[Tuple[pd.Timestamp, pd.Timestamp]]:
    """Return contiguous frozen-soil spans derived solely from soil temperature."""

    if df.empty or date_col not in df or temp_col not in df:
        return []

    working = df[[date_col, temp_col]].dropna()
    if working.empty:
        return []

    # Aggregate to daily minima so a single below-threshold reading flags the day.
    daily_temp = (
        working.sort_values(date_col)
        .set_index(date_col)[temp_col]
        .resample('D', label='left', closed='left')
        .median()
    )
    if daily_temp.empty:
        return []

    freeze_mask = daily_temp <= threshold
    if not freeze_mask.any():
        return []

    date_series = pd.Series(daily_temp.index, index=daily_temp.index)
    return _contiguous_date_ranges(date_series, freeze_mask)


def summarize_crop_station_years(
    test_df: pd.DataFrame,
    *,
    station_col: str = "station",
    year_col: str = "year",
    crop_col: str = "lc",
) -> pd.DataFrame:
    """Return a pivot table of counts by station/year for each crop type."""

    required = [station_col, year_col, crop_col]
    missing = [col for col in required if col not in test_df.columns]
    if missing:
        raise KeyError(f"Required columns missing from dataframe: {missing}")

    working = test_df[required].dropna()
    if working.empty:
        raise ValueError("No data available after dropping rows with missing station/year/crop values.")

    working["_count"] = 1
    pivot = (
        working.pivot_table(
            index=[station_col, year_col],
            columns=crop_col,
            values="_count",
            aggfunc="count",
            fill_value=0,
        )
        .sort_index()
        .sort_index(axis=1)
    )
    pivot.index = pivot.index.set_names([station_col, year_col])
    pivot.columns.name = crop_col
    return pivot


def summarize_crop_coverage(
    test_df: pd.DataFrame,
    *,
    station_col: str = "station",
    year_col: str = "year",
    crop_col: str = "lc",
) -> pd.DataFrame:
    """Summarise availability of crops across stations and years."""

    required = [station_col, year_col, crop_col]
    missing = [col for col in required if col not in test_df.columns]
    if missing:
        raise KeyError(f"Required columns missing from dataframe: {missing}")

    df = test_df[required].dropna()
    if df.empty:
        raise ValueError("No data available after dropping rows with missing station/year/crop values.")

    unique_triplets = df.drop_duplicates([station_col, year_col, crop_col])

    records = df.groupby(crop_col).size()
    station_years = unique_triplets.groupby(crop_col).size()
    stations = unique_triplets.groupby(crop_col)[station_col].nunique()
    years = unique_triplets.groupby(crop_col)[year_col].nunique()

    summary = pd.DataFrame(
        {
            "records": records,
            "station_year_pairs": station_years,
            "unique_stations": stations,
            "unique_years": years,
        }
    ).fillna(0).astype(int).sort_index()

    return summary
